
# ğŸ§ ğŸ’¡ Text-to-Image Generation Pipeline using Chain-of-Thought and Diffusion Models

This project implements a **Text-to-Image Generation Pipeline** that first enhances user prompts through a **Chain-of-Thought (CoT) reasoning model**, and then generates corresponding images using a **Diffusion-based generative model**. The pipeline is designed for creative applications such as storytelling, concept visualization, and generative AI tasks.

---

## ğŸ“Œ Features

- **ImprovedChainOfThought** model for expanding prompts using step-by-step reasoning.
- **CLIP-based tokenizer and encoder** to transform text into latent space.
- **VAE Encoderâ€“Decoder** structure for image representation and generation.
- **DDPM Sampler** (Denoising Diffusion Probabilistic Models) for high-quality image synthesis.
- Configurable pipeline: supports CFG scale, strength, inference steps, and random seed control.

---

## ğŸ” Model Explanation

| Component        | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| **CoT Model**    | A Transformer-based model that generates detailed prompts using Chain-of-Thought reasoning to improve image relevance. |
| **CLIP**         | Converts text into a latent embedding space compatible with the diffusion model. |
| **VAE Encoder**  | Compresses image features into a latent representation.                     |
| **VAE Decoder**  | Reconstructs images from latent vectors after sampling.                     |
| **DDPM**         | Diffusion-based image generator that denoises latent space into realistic images. |

---

## ğŸ§© Architecture Overview

```
User Prompt
   â†“
[Chain-of-Thought Reasoning]
   â†“
Refined Prompt (e.g., â€œA dog eating a hotdog in Central Parkâ€)
   â†“
[CLIP Tokenizer & Encoder]
   â†“
[Diffusion-based Image Generator]
   â†“
Generated Image Output (RGB)
```

---

## ğŸ”§ Setup Instructions

### 1. Install Dependencies

```bash
pip install torch torchvision transformers Pillow
```

### 2. Download Pretrained Models

#### ğŸ“¥ Get Stable Diffusion v1.5 Checkpoint

- Visit: [Stable Diffusion v1-5 Hugging Face](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/tree/main)
- Download file: `v1-5-pruned-emaonly.ckpt`
- Save it to:  
  ```bash
  diffusion_model/data/v1-5-pruned-emaonly.ckpt
  ```

### 3. Required Model Files

Ensure the following files exist:

```
model.pth                   # Chain-of-Thought model checkpoint
diffusion_model.pth         # DDPM model weights
vae_encoder.pth             # VAE encoder weights
vae_decoder.pth             # VAE decoder weights
clip_model.pth              # CLIP model weights
```

> âš ï¸ If these files are missing, the pipeline will fallback to untrained models and results may be poor.

---

## ğŸš€ Running the Pipeline

```bash
python pipeline.py
```

This will:
- Accept a user prompt
- Expand it using Chain-of-Thought reasoning
- Generate an image using the diffusion model
- Save the image as `output.png`

---

## ğŸ“ File Structure

```
.
â”œâ”€â”€ pipeline.py
â”œâ”€â”€ Chain_Of_Thought/
â”‚   â””â”€â”€ COT_text_gen.py
â”œâ”€â”€ diffusion_model/
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ clip.py
â”‚   â”œâ”€â”€ diffusion.py
â”‚   â”œâ”€â”€ ddpm.py
â”‚   â”œâ”€â”€ VAen_decoder.py
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ v1-5-pruned-emaonly.ckpt   # Stable Diffusion weights
â”œâ”€â”€ model.pth
â”œâ”€â”€ output.png
```

---

## ğŸ“š License

This project is intended for **research and educational purposes** only. Contact the author for commercial use.

---

## ğŸ‘¨â€ğŸ’» Author

**[Your Name]**  
AI Researcher / Developer  
ğŸ“« [your-email@example.com] | GitHub: [your-profile]
